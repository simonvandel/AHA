%mainfile: ../master.tex
\chapter{Test and Verification}
This section contains a description of the testing, performed on the system to verify that it, to a satisfying degree, solves the problem solution. Along with this description, a number of alternatives will be discussed in relation to why they were not chosen.

\section{Methodology}
To test the system, we set up a simple test system in a controlled environment, consisting of an embedded subsystem with 2 non-emulatable boolean sensors and 1 emulatable boolean sensor connected, and the learning subsystem. Here the system is exposed to deliberate patterns, designed to test the ability to learn, timing and adaptability of the system. A pattern here consists of an action and a specific sequence of sensor snapshots. The patterns were performed until the system took over and performed the actions accordingly to the patterns performed, at which the test would be considered successful, or until the confidence of the reasoner did not change in a positive direction for the given pattern. This is a simple test, but it shows that our system can recognise a pattern, which was deemed to be the smallest requirement for the system to be working properly.

For testing whether the system satisfies the deadline of 100 ms, as expressed in \cref{sub:people}, the time it takes from reading the sensors till an action is performed must be measured. This is done by inserting timing code in the embedded subsystem code. The timer starts when sensors are read and stops when an action is performed. Doing this multiple times and averaging of the resulting timings, gives us an indication of the average time it takes to respond to some interaction. This is however not the worst case execution time of the system. Some theoretical worst case analysis of the communication can be seen in \cref{sec:xbee} and for the embedded subsystem in \cref{sub:sensorStation}. The learning subsystem is hard to reason about, as we do not have control over the scheduler, so no theoretical analysis exists for the learning subsystem.

Once verified that the system can learn a usage pattern, reason and subsequently perform the appropriate action for a given pattern within the deadline, the second key component of the system is tested; the ability to adapt to change in the patterns. After the system had learned a pattern, a different overlapping pattern is performed, until the system adapts to it.

\section{Results}
\input{Verification/tests.tex}

\section{Adaptation test}
A second test were conducted. In this the adaptation of the system were tested. The system should be able to change patterns and take feedback from the user. First the XOR pattern as described in \cref{subsec:singlePatternTest} were learned. After this the user tried to override this with a AND pattern. This resulted in the user inverting the actions as can be seen in \cref{Table:userPatternChange}. This should be red as what the system is currently enforcing on the left side of the arrow (i.e what is true in XOR) and what the user then would enforce instead on the system on the right side of the arrow. This results in the user inverting the action of the system in three of the cases and only leaving one case (0,0) left as it is. 


\begin{table}[htbp]
\begin{center}
  \begin{tabular}{| c | c | c |}
    \hline
        & 0                 & 1                 \\ \hline
      0 & $0 \rightarrow 0$ & $1 \rightarrow 0$ \\ \hline
      1 & $1 \rightarrow 0$ & $0 \rightarrow 1$ \\
    \hline
  \end{tabular}
  \caption{Table illustrating change in the users pattern}\label{Table:userPatternChange}
\end{center}
\end{table}

Some errors were found with the feedback method.
The system did not remove the wrong action the first time they were registered as it should.
The method did not always register feedback from the user due to errors in the timing in the AI subsystem.

It was found that the system is better at finding emerging patterns than adapting to new patterns. This also makes sense when considering how the learning and reasoner has been implemented. The Baum Welch algorithm has been created to find emerging patterns in data. Normally the problem domain is not aware that it is being monitored and a change in pattern will only be show indirectly through a change in the patterns done by the problem and the systems preference would only change when the amount of proofs for the new patterns exceeded the old pattern.

\section{Alternative tests}
The first alternative which was considered is a real world use case. In this test a system would be setup in one or more users homes for a given set of time. Once the test had finished the log data from the system along with user testimony would determine the performance of the system. This test is time consuming, because the system would need to be live for an extended period of time to learn user patterns on a daily, weekly, or longer scale. The log data can also be difficult and time consuming to analyse because the large amount of data processed by the system and the difficulty to visually represent the machine learning model used.
\\\\
Unit tests is another alternative which ensures some stability in the system. But for a system that relies on random data with patterns, which is difficult to produce programmatically, unit tests do not show how well the system performs, only that it produces some data. A preliminary unit test was performed on the system. In this test the system was given a specific set of sensor states a set number of times and then checked whether the system produced the expected action given a sensor state.
