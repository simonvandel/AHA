%mainfile: ../master.tex
\chapter{Test and Verification}
This section contains a description of the testing, performed on the system to verify that it, to a satisfying degree, solves the problem solution. Along with this description, a number of alternatives will be discussed in relation to why they were not chosen.

\section{Methodology}
To test the system, we set up a simple test system in a controlled environment, consisting of an embedded subsystem with 2 non-emulatable boolean sensors and 1 emulatable boolean sensor connected, and the learning subsystem. Here the system is exposed to deliberate patterns, designed to test the ability to learn, timing and adaptability of the system. A pattern here consists of an action and a specific sequence of sensor snapshots. The patterns were performed until the system took over and performed the actions accordingly to the patterns performed, at which the test would be considered successful, or until the confidence of the reasoner did not change in a positive direction for the given pattern. This is a simple test, but it shows that our system can recognise a pattern, which was deemed to be the smallest requirement for the system to be working properly.

For testing whether the system satisfies the deadline of 100 ms, as expressed in \cref{sub:people}, the time it takes from reading the sensors till an action is performed must be measured. This is done by inserting timing code in the embedded subsystem code. The timer starts when sensors are read and stops when an action is performed. Doing this multiple times and averaging of the resulting timings, gives us an indication of the average time it takes to respond to some interaction. This is however not the worst case execution time of the system. Some theoretical worst case analysis of the communication can be seen in \cref{sec:xbee} and for the embedded subsystem in \cref{sub:sensorStation}. The learning subsystem is hard to reason about, as we do not have control over the scheduler, so no theoretical analysis exists for the learning subsystem.

Once verified that the system can learn a usage pattern, reason and subsequently perform the appropriate action for a given pattern within the deadline, the second key component of the system is tested; the ability to adapt to change in the patterns. After the system had learned a pattern, a different overlapping pattern is performed, until the system adapts to it.

\section{Results}
\input{Verification/tests.tex}

\section{Alternative tests}
The first alternative which was considered is a real world use case. In this test a system would be setup in one or more users homes for a given set of time. Once the test had finished the log data from the system along with user testimony would determine the performance of the system. This test is time consuming, because the system would need to be live for an extended period of time to learn user patterns on a daily, weekly, or longer scale. The log data can also be difficult and time consuming to analyse because the large amount of data processed by the system and the difficulty to visually represent the machine learning model used.
\\\\
Unit tests is another alternative which ensures some stability in the system. But for a system that relies on random data with patterns, which is difficult to produce programmatically, unit tests do not show how well the system performs, only that it produces some data. A preliminary unit test was performed on the system. In this test the system was given a specific set of sensor states a set number of times and then checked whether the system produced the expected action given a sensor state.
