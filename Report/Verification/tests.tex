All the logs can be found in \cref{app:cd}. \ranote{remember to put the logs on the CD}
\subsubsection{Single Pattern Test}\label{subsec:singlePatternTest}
The setup consisted of a single sensor station with 3 switches. The switches were connected to LEDs, such that each switch controlled one LED. The pattern being performed is XOR. 2 switches represents the boolean values for the XOR funciton, and the last switch, which is emulatable by the system, represents the result of applying XOR to the value of the two switches.

A snippet of the samples this test produced can be seen in \cref{Table:SampleSnippet}, where time moves from left to right so the rightmost state is the most recent.
\begin{center}

\begin{table}[htbp]
  \centering
  \begin{tabular}{c c c c c c c c c c c}
    \toprule
    Sample & & & & & & & & & &  \\ \midrule
           1 & 7 & 11 & 10 & 6 & 8 & 12 & 13 & 9 & 7 & 11 \\
           2 & 11 & 10 & 6 & 8 & 12 & 13 & 9 & 7 & 11 & 10 \\
           3 & 10 & 6 & 8 & 12 & 13 & 9 & 7 & 11 & 10 & 6 \\
           4 & 6  & 8 & 12 & 13 & 9 & 7 & 11 & 10 & 6 & 7 \\
           5 & 8 & 12 & 13 & 9 & 7 & 11 & 10 & 6 & 7 & 11 \\
           6 & 12 & 13 & 9 & 7 & 11 & 10 & 6 & 7 & 11 & 13 \\
           7 & 13 & 9 & 7 & 11 & 10 & 6 & 7 & 11 & 13 & 9 \\
           8 & 9 & 7 & 11 & 10 & 6 & 7 & 11 & 13 & 9 & 8 \\
           9 & 7 & 11 & 10 & 6 & 7 & 11 & 13 & 9 & 8 & 6 \\
           10& 11 & 10 & 6 & 7 & 11 & 13 & 9 & 8 & 6 & 10 \\
           11 & 10 & 6 & 7 & 11 & 13 & 9 & 8 & 6 & 10 & 6
     \\ \bottomrule
  \end{tabular}
  \caption{Samples from test with simple pattern, as performed by the user}\label{Table:SampleSnippet}
\end{table}
\end{center}
Once 32 samples had been received, as can be seen in a snippet from the ai log \cref{Listing:MarkovGenLog}, the learner was run and a model generated.

\lstset{language=xml}
\begin{lstlisting}[label = Listing:MarkovGenLog, caption = Snippet of log from model generation]
  <date>2015-12-17T20:13:01</date>
  <millis>1450379581194</millis>
  <logger>aiLogger</logger>  
  <message>Sample size for generating hidden markov model: 32</message>
\end{lstlisting}

With this model, the system started trying to predict actions. At this point, no user actions were being performed on sensor id 1. The confidence threshold for this test was 60\%. The log trace for the first action can be seen in \cref{Listing:CompletActionTrace}. As can be seen in the logs, the system predicted the state at 20:13:07, based on the state from 20:13:04. Looking at the logs before that, shows user actions; it can be seen that this pattern was repeated by the user.

\begin{lstlisting}[label = Listing:CompletActionTrace, caption = Snippets from different logs to show how the process of making an action]
    <record>
        <date>2015-12-17T20:13:04</date>
        <millis>1450379584477</millis>
        <logger>sampleLogger</logger>
        <thread>1</thread>
        <message>Sample: 9 8 6 10 6 7 9 7 11 13</message>
    </record>

    <record>
        <date>2015-12-17T20:13:04</date>
        <millis>1450379584601</millis>
        <logger>aiLogger</logger>
        <thread>1</thread>
        <message>Confidence: 0.6542340925943608. Actions: 
            Set sensor id 2 to value 1</message>
    </record>

    <record>
        <date>2015-12-17T20:13:04</date>
        <millis>1450379584601</millis>
        <logger>reasonLogger</logger>
        <message>Sending data: Set sensor id 2 to value 1</message>
    </record>

    <record>
        <date>2015-12-17T20:13:07</date>
        <millis>1450379587647</millis>
        <logger>sampleLogger</logger
        <message>Sample: 8 6 10 6 7 9 7 11 13 9</message>
    </record>
\end{lstlisting}

Based on this it can be concluded that the system successfully learned and imitated a pattern.

\subsubsection{Deadline Test}
To perform this test, the system was first taught the XOR pattern, as it was important that the system could predict an action during the test. A button was added, that had no other function than to start the test. This allowed the test code to artificially create an action on the button and then immediately start a timer. The button was then disabled, in order to avoid restarting the timer. Then, once the embedded subsystem received the action, the timer was stopped, and the time for a full loop of the system was found. One thing to note is that this time can vary, depending on when in the embedded subsystem's loop, the button was pressed.

To document this, two tests were performed; one, where the artificial button press happened at the theoretically optimal time, and one at the theoretically worst time. The worst case happens when we perform the button press after we send a snapshot to the learning subsystem, but before we start receiving data. This means that we have to wait for the delay of receiving, then sending again, and then wait for the delay of receiving again.

As can be seen below, the system does not uphold the deadline. This could be attributed to the sending and receiving times of the XBee modules which, in practical testing, showed a delay of up to 60 ms for one send, however, theoretically, the worst case should be 50 ms\cite{xbee_latency}. This variance could be due to a number of things, for example cheap hardware or a non-optimal implementation. The average time spent in the learning subsystem for both the simple pattern test and the deadline test, was 52.5 ms. This was found by averaging over all runs described in the logs that can be found in \cref{app:cd}.

For the best case test, the lowest observed time was 122 ms, which is 22 ms above the deadline. For each separate module, we have the approximate running time, and so an approximation of a worst case full loop of the system would be 190 ms as seen in \cref{Table:RunTimeAprox}.
\begin{center}
	\begin{table}[htbp]
	  \centering
	  \begin{tabular}{l l}
		\toprule
		Position in loop		& Run Time  \\ \midrule
		Test Start		        & 0 ms         \\ \midrule
		Encode 			        & <2 ms  	\\ \midrule
		Send   			        & <50 ms     \\ \midrule
		Learner loop 	        & <88 ms     \\ \midrule
		Receive 		        & <50 ms     \\ \midrule \midrule
		Total			        & <190 ms	    \\
                                            \bottomrule
	  \end{tabular}
	  \caption{An approximation of a worst case run time of each separate module, when performing an action at the optimal time in the Arduino loop.}\label{Table:RunTimeAprox}
	\end{table}
\end{center}

The best observed run time, in the worst case test, was 268 ms. The approximation of a worst case, full loop of the system, in this situation would be 378 ms as seen in \cref{Table:WorstRunTimeAprox}

\begin{center}
	\begin{table}[htbp]
	  \centering
	  \begin{tabular}{l l}
		\toprule
		Position in loop		& Run Time  \\ \midrule
		Encode 			        & 0  	  	\\ \midrule
		Test Start		        & 0  		\\ \midrule
		Send   			        & <50 ms     \\ \midrule
		Learner loop 	        & <88 ms     \\ \midrule
		Receive 		        & <50 ms     \\ \midrule
		encode 			        & <2 ms      \\ \midrule
		send   			        & <50 ms     \\ \midrule
		Learner loop 	        & <88 ms	    \\ \midrule
		Receive 		        & <50 ms     \\ \midrule \midrule
		Total			        & <378 ms     \\
                                            \bottomrule
	  \end{tabular}
	  \caption{An approximation of a worst case run time of each separate module, when performing an action at the worst time in the Arduino loop.}\label{Table:WorstRunTimeAprox}
	\end{table}
\end{center}

These tests show that the system has a certain risk of exceeding the deadline, by up to 278 ms. So even though the system can get close to the deadline, in a best case upholding it, it cannot be guaranteed. However according to \cref{jakobnielsen} exceeding the deadline by 278 ms would still not cause any annoyance to the user, since the time is unnoticeable.

\subsubsection{Adaption Test}
