\section{Hidden Markov Model}
\label{sec:hmm}
In this project hidden Markov models (HMMs) are used to model a probability distribution over behavioural patterns, that will be used to predict user behaviours. HMMs are typically used when reasoning over time, as they incorporate discrete time changes. 

The following section describes the components of a HMM, and how we solve the problem of predicting, that is, given a sequence of observations up to time $t$ in the problem domain, what is the most likely observation in time $t+1$?

HMMs have been chosen among other mathematical structures used for prediction, because they have seen many applications in temporal pattern recognition. If the focus in this project was finding the best possible way to predict user actions in the fastest, most precise way, multiple mathematical structures would have to be compared based on real observation data.

The known, or observable, data in this project are what the sensors observe. 
An HMM can be graphically represented by means of a trellis diagram\cite{russell2010artificial}. The simplest form of a Markov model can be seen in \cref{fig:1stMarkovModel}. Here we can see the property that at any state in time $s_t$ the next discrete time future $s_{t+1}$ is only dependent on the current state $s_t$, and not on previous states. This is called the \emph{Markov property}. But this Markov property is also means that if we want to predict the next state $t+1$, just based on sensor values in states $t$, we could only choose the state that is most occurring after the state at time $t$, this is not preferable in the context of this system, because some states might be equal in terms of sensor values but can lead to multiple different states depending on how we arrived at state at time $t$, therefore we want to find relations between states not between sensor values. This is also discussed in this paper\cite{Allahviranloo201316} by Mahdieh Allahviranloo and Will Recker, it mentions that state $t+1$ is potentially dependent on the whole history of states. This however is overcome by instead of the sensor values being the input features, a state and some of the states prior to that state, its history, is the input features. The effect of this is instead of reasoning between sensor values at in the state at time $t$, the learner can reason on how we arrived at the state at time $t$. Thereby encoding the some history $t-n$ of the state $t$ in state $t$ itself.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  %World States Nodes
  \node          (dots) [draw=none,  minimum width=1.25cm, minimum height=.75cm] {\LARGE \dots};
  \node[ellipse] (htp2) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of dots] {$s_{t-2}$};
  \node[ellipse] (htp1) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of htp2] {$s_{t-1}$};
  \node[ellipse] (ht)   [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of htp1] {$s_t$};
  \node[ellipse] (htf1) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of ht] {$s_{t+1}$};

  %1. order transition lines
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=0,in=180] (htp2);
  \draw [->, to path={-| (\tikztotarget)}] (htp2) edge[out=0,in=180] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=0,in=180] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=0,in=180] (htf1);
\end{tikzpicture}
\caption[Trellis diagram for a simple Markov model]{The trellis diagram for a hidden Markov model with discrete time world state changes}\label{fig:1stMarkovModel}
\end{figure}

Even though these diagrams are good at illustrating the models and how they change over time, it is important to remember that these are not illustrating the exact models themselves, only how they change over time for a specific sequence of observations. The actual model could be illustrated with causal networks, which will be used later.

A specific user action cannot be justified only based on sensor data, e.g. the fact that the light should be switched on does not only depend on the fact that the user is present in the room, but also on the fact that the user is awake. In this example, the information whether the user is awake, is hidden. To represent this data, HMMs use the concept of hidden states. This data can not be observed, only assumed.

The hidden states of our hidden markov model are all the possible samples we can construct from all the possible snapshots of size(history) 
$D(hiddenStates) = (D(sensor_1)\times D(sensor_2)\times ... \times D(sensor_n))^size(history)$. \cref{fig:2ndMarkovModel} is an example of a simple HMM. Note that a trellis diagram represents a sequence of observed states over time.

We denote by $E=\{e_1, \dots ,e_{M}\}$ the set of emission states and $\mathcal{O}=\{o_1, \dots , o_T\}$ the set of observed states, where $M$ and $T$ are respectively the number of emission states and observations.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  %Hidden Nodes
  \node          (dots) [draw=none,  minimum width=1.25cm, minimum height=.75cm] {\LARGE \dots};
  \node[ellipse] (htp2) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of dots] {$x_{t-2}$};
  \node[ellipse] (htp1) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of htp2] {$x_{t-1}$};
  \node[ellipse] (ht)   [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of htp1] {$x_t$};
  \node[ellipse] (htf1) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of ht] {$x_{t+1}$};

  %Emission states
  \node[ellipse] (etp1) [draw=black, minimum width=1.25cm, minimum height=.75cm, below=of htp1] {$e_{t-1}$};
  \node[ellipse] (etp2) [draw=black, minimum width=1.25cm, minimum height=.75cm, below=of htp2] {$e_{t-2}$};
  \node[ellipse] (et)   [draw=black, minimum width=1.25cm, minimum height=.75cm, below=of ht] {$e_t$};
  \node[ellipse] (etf1) [draw=black, minimum width=1.25cm, minimum height=.75cm, below=of htf1] {$e_{t+1}$};

  %Separator
  \coordinate (MW) at (0,-0.8);
  \node (HW) [above left=0ex of MW] {Hidden world $\uparrow$};
  \node (VW) [below left=0ex of MW] {Observable world $\downarrow$};
  \draw[dashed, draw=black] (-1,-0.8) -- (10.5,-0.8);
  %\draw[dashed] ($(htp2)!0.5!(etp2)$) -- ($(htf1)!0.5!(etf1)$);

  %Emission lines
  \draw[->, to path={-| (\tikztotarget)}] (htp1) edge[out=-90,in=90] (etp1);
  \draw[->, to path={-| (\tikztotarget)}] (htp2) edge[out=-90,in=90] (etp2);
  \draw[->, to path={-| (\tikztotarget)}] (ht) edge[out=-90,in=90] (et);
  \draw[->, to path={-| (\tikztotarget)}] (htf1) edge[out=-90,in=90] (etf1);

  %1. order lines
  \draw[->, to path={-| (\tikztotarget)}] (dots) edge[out=0,in=180] (htp2);
  \draw[->, to path={-| (\tikztotarget)}] (htp2) edge[out=0,in=180] (htp1);
  \draw[->, to path={-| (\tikztotarget)}] (htp1) edge[out=0,in=180] (ht);
  \draw[->, to path={-| (\tikztotarget)}] (ht) edge[out=0,in=180] (htf1);
\end{tikzpicture}
\caption[Trellis diagram for a simple Hidden Markov Model]{The trellis diagram for a simple hidden Markov model that assumes the Markov property}\label{fig:2ndMarkovModel}
\end{figure}

As described in \cref{sec:sampler}, the Markov property might not model the reality accurately. One could increase the number of states a state is dependent on in the past. The order of a HMM directly corresponds to how many states any state $s_t$ is dependent on. This is illustrated in \cref{fig:3rdMarkovModel} for an HMM of order 2.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  %Hidden Nodes
  \node          (dots) [draw=none,  minimum width=1cm, minimum height=.75cm] {\LARGE \dots};
  \node[ellipse] (htp2) [draw=black, minimum width=1cm, minimum height=.75cm, right=of dots] {$x_{t-2}$};
  \node[ellipse] (htp1) [draw=black, minimum width=1cm, minimum height=.75cm, right=of htp2] {$x_{t-1}$};
  \node[ellipse] (ht)   [draw=black, minimum width=1cm, minimum height=.75cm, right=of htp1] {$x_t$};
  \node[ellipse] (htf1) [draw=black, minimum width=1cm, minimum height=.75cm, right=of ht] {$x_{t+1}$};

  %Emission states
  \node[ellipse] (etp2) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htp2] {$e_{t-2}$};
  \node[ellipse] (etp1) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htp1] {$e_{t-1}$};
  \node[ellipse] (et)   [draw=black, minimum width=1cm, minimum height=.75cm, below=of ht] {$e_t$};
  \node[ellipse] (etf1) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htf1] {$e_{t+1}$};

  %Emission lines
  \draw [->, to path={-| (\tikztotarget)}] (htp2) edge[out=-90,in=90] (etp2);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=-90,in=90] (etp1);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=-90,in=90] (et);
  \draw [->, to path={-| (\tikztotarget)}] (htf1) edge[out=-90,in=90] (etf1);

  %1. order lines
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=0,in=180] (htp2);
  \draw [->, to path={-| (\tikztotarget)}] (htp2) edge[out=0,in=180] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=0,in=180] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=0,in=180] (htf1);

  %2. order lines
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=45,in=135] (htp2);
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=45,in=135] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (htp2) edge[out=45,in=135] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=45,in=135] (htf1);
\end{tikzpicture}
\caption[Trellis diagram for a 2nd order HMM]{A trellis diagram for a 2nd order HMM.}\label{fig:3rdMarkovModel}
\end{figure}

Here, we can see that every state $s_t$ is dependent on two states in the past, $s_{t-1}$ and $s_{t-2}$, meaning that every state is not only dependent on what happened the time step before, violating the \emph{Markov property}. This can trivially be extended to a HMM of any order. Any $n^{th}$ order Markov model, where $n>1$, violates the Markov property. Many algorithms within this field assumes the Markov property, including the ones later used in this project. Any $n^{th}$ order Markov model can be converted to a 1st order Markov model, thereby keeping the Markov property. This is done in this project, by encoding the past, in form of the prior hidden states, in the hidden state. The sampler is described in \cref{sec:sampler}.

On the basis of this, we can now find the specific hidden states. We introduce the term \emph{sample} that covers the current observable snapshot together with the history of snapshots $n$ steps back. The chosen value of $n$ will be discussed in \cref{sec:sampler}. This sample is used to find hidden states. A list of samples are created from an observation sequence of snapshots, further described in \cref{sec:sampler}. One could argue that this sample needs to be created from a history of hidden states, not snapshots, since it is these we depend on in the current state. A compromise was therefore made so that the samples, and thereby hidden states, are found from the observations and history of observations, on the reasoning that we cannot distinguish between two hidden states from anything else then the observations. A hidden state will therefore be created for every sample in the HMM to keep the model in 1st order and thereby keep the Markov property.

In a visual representation of a HMM a state is connected with a direction to other states with a certain probability. This denotes the probability of transitioning from one state to another. There are two different forms of transitions in such a model.

First, there is the hidden state transition from one state to another, here denoted as $a_{ij}$, that represents the probability that at time $t$ the hidden state is $x_j$, given that at time $t-1$ the hidden state is $x_i$.

Secondly, we have the transition from a hidden state to an emission state. This is called emitting an emission state. It is denoted by $b_{ij}$, meaning emitting emission state $e_j$ in hidden state $x_i$. This is associated with a certain probability.

Furthermore, we have the initial distribution. This denotes the probability distribution of any hidden state without a prior hidden state; the initial probability for being in hidden state $x_i$. This can essentially be described as the probability of starting in that state.

All of these types of transitions and probabilities can be described in matrices, which brings us to the formal definition of a HMM:

\begin{align*}
\theta = \{A,B,\pi\} &|\\
A &= \text{hidden state transition matrix}\\
B &= \text{emission matrix}\\
\pi &= \text{initial distribution}
\end{align*}

Here we can see that we have a matrix for each form of probability. This means that each matrix contains probabilities. The transition matrix $A$ is the matrix denoting the hidden state transition probability $P(X_t=x_j|X_{t-1}=x_i)$. The emission matrix $B$ denotes the emission probability, $P(E_t=e_j|X_t=x_i)$. We can also visually represent a HMM in a causal network, as can be seen in \cref{fig:HMMCausalNet}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[->]
  %Hidden Nodes
  \node[circle, draw=black, minimum size=1cm] (x1) {$x_1$};
  \node[circle, draw=black, minimum size=1cm, node distance=3cm, right=of x1] (x2) {$x_2$};
  \node[circle, draw=black, minimum size=1cm, node distance=3cm, below=of x1] (x3) {$x_3$};
  \node[circle, draw=black, minimum size=1cm, node distance=3cm, below=of x2] (x4) {$x_4$};

  %Emission states
  \node[circle] (e11) [draw=black, minimum size=1cm, node distance=1.5cm, above=of x1] {$e_1$};
  \node[circle] (e12) [draw=black, minimum size=1cm, node distance=1.5cm, left=of x1] {$e_2$};
  \node[circle] (e21) [draw=black, minimum size=1cm, node distance=1.5cm, above=of x2] {$e_1$};
  \node[circle] (e22) [draw=black, minimum size=1cm, node distance=1.5cm, right=of x2] {$e_2$};
  \node[circle] (e31) [draw=black, minimum size=1cm, node distance=1.5cm, left=of x3] {$e_1$};
  \node[circle] (e32) [draw=black, minimum size=1cm, node distance=1.5cm, below=of x3] {$e_2$};
  \node[circle] (e41) [draw=black, minimum size=1cm, node distance=1.5cm, below=of x4] {$e_1$};
  \node[circle] (e42) [draw=black, minimum size=1cm, node distance=1.5cm, right=of x4] {$e_2$};

  \path
  %Emission lines
    (x1) edge[out=90,in=-90] node[rectangle,fill=white] {$b_{11}$} (e11)
    (x1) edge[out=180,in=0] node[rectangle,fill=white] {$b_{12}$} (e12)
    (x2) edge[out=90,in=-90] node[rectangle,fill=white] {$b_{21}$} (e21)
    (x2) edge[out=0,in=180] node[rectangle,fill=white] {$b_{22}$} (e22)
    (x3) edge[out=180,in=0] node[rectangle,fill=white] {$b_{31}$} (e31)
    (x3) edge[out=-90,in=90] node[rectangle,fill=white] {$b_{32}$} (e32)
    (x4) edge[out=-90,in=90] node[rectangle,fill=white] {$b_{41}$} (e41)
    (x4) edge[out=0,in=180] node[rectangle,fill=white] {$b_{42}$} (e42)

  %Transition lines
    (x1) edge[out=15,in=165] node[rectangle,fill=white] {$a_{12}$} (x2)
    (x2) edge[out=-165,in=-15] node[rectangle,fill=white] {$a_{21}$} (x1)
    (x2) edge[out=-75,in=75] node[rectangle,fill=white] {$a_{24}$} (x4)
    (x4) edge[out=105,in=-105] node[rectangle,fill=white] {$a_{42}$} (x2)
    (x4) edge[out=-165,in=-15] node[rectangle,fill=white] {$a_{43}$} (x3)
    (x3) edge[out=15,in=165] node[rectangle,fill=white] {$a_{34}$} (x4)
    (x3) edge[out=105,in=-105] node[rectangle,fill=white] {$a_{31}$} (x1)
    (x1) edge[out=-75,in=75] node[rectangle,fill=white] {$a_{13}$} (x3)

    (x1) edge[out=-30,in=120] node[rectangle,fill=white] {$a_{14}$} (x4)
    (x4) edge[out=150,in=-60] node[rectangle,fill=white] {$a_{41}$} (x1)
    (x3) edge[out=60,in=-150] node[rectangle,fill=white] {$a_{32}$} (x2)
    (x2) edge[out=-120,in=30] node[rectangle,fill=white] {$a_{23}$} (x3)

    (x1) edge [out=150,in=120,loop,looseness=15] node[rectangle,fill=white] {$a_{11}$} (x1)
    (x2) edge [out=60,in=30,loop,looseness=15] node[rectangle,fill=white] {$a_{22}$} (x2)
    (x3) edge [out=-120,in=-150,loop,looseness=15] node[rectangle,fill=white] {$a_{33}$} (x3)
    (x4) edge [out=-30,in=-60,loop,looseness=15] node[rectangle,fill=white] {$a_{44}$} (x4);
\end{tikzpicture}
\caption[A HMM represented as a causal network.]{A hidden Markov model, with four hidden states and two emission states, represented as a causal network.}\label{fig:HMMCausalNet}
\end{figure}

\Cref{fig:HMMCausalNet} represents a HMM with four hidden states and two emission states. The names on the arrows in the matrix (e.g. $a_{ij}$ and $b_{ij}$) represent entries in the two matrices, i.e. probabilities. We can see that every hidden state is connected to every hidden state (including itself) and that every hidden state can emit every emission state. This is not always the case in reality, since some hidden states could never directly reach other states and not every emission state could be emitted from every hidden state. This is represented as a matrix with entries with the value 0. Arcs corresponding to transition probabilities with probability 0, are omitted in the graphical representation.

The learning algorithms for HMMs cannot learn the connections between states, meaning that if there is no connection to begin with, there never will be one in the future. This is a strong assumption to make, since we rarely have a complete understanding of the problem domain (after all, it is why we are trying to learn the hidden states) nor have a complete understanding of what each state exactly represents. There are however some connections that cannot happen in the problem domain, so these connections would have a probability of $0$. However, there are no impossible connections in this project. Hidden states can emit every emission state, as unreliable sensors could represent an impossible emission state.\sinote{Jens, kig pÃ¥ det her, og skriv det om} Furthermore, addressing the transitions between hidden states, the hidden states are partly based on the user's memory and how the user thinks. This can change from one state to another at will, even if the transition is physically impossible\kanote{same with this point}. We assume that the user remembers the events in the past, as they happened, but since a user can forget, see reality differently or our sensors observe wrongly, we leave all connections in the model and thereby making it able to accommodate changes and unreliability.

In the formal definition of HMMs we also had $\pi$, a matrix denoting the initial hidden state probability distribution, $P(X_1=x_i)$. Until now we have only illustrated models with relative time, but the initial probabilities give us the notion that there is an absolute time, starting at time 1 with probabilities denoted in this table. This is needed, since any observation sequence must have a starting point, and is going to become very important later, when we will learn and reason about the model. We can now expand the trellis diagrams with the notion of a starting point as can be seen in \cref{fig:4thMarkovModel}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  %Hidden Nodes
  \node[ellipse] (h1)   [draw=black, minimum width=1cm, minimum height=.75cm] {$x_1$};
  \node[ellipse] (h2)   [draw=black, minimum width=1cm, minimum height=.75cm, right=of h1] {$x_2$};
  \node          (dots) [draw=none,  minimum width=1cm, minimum height=.75cm, right=of h2] {\LARGE \dots};
  \node[ellipse] (htp1) [draw=black, minimum width=1cm, minimum height=.75cm, right=of dots] {$x_{t-1}$};
  \node[ellipse] (ht)   [draw=black, minimum width=1cm, minimum height=.75cm, right=of htp1] {$x_t$};
  \node[ellipse] (htf1) [draw=black, minimum width=1cm, minimum height=.75cm, right=of ht] {$x_{t+1}$};

  %Emission states
  \node[ellipse] (e1)   [draw=black, minimum width=1cm, minimum height=.75cm, below=of h1] {$e_1$};
  \node[ellipse] (e2)   [draw=black, minimum width=1cm, minimum height=.75cm, below=of h2] {$e_2$};
  \node[ellipse] (etp1) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htp1] {$e_{t-1}$};
  \node[ellipse] (et)   [draw=black, minimum width=1cm, minimum height=.75cm, below=of ht] {$e_t$};
  \node[ellipse] (etf1) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htf1] {$e_{t+1}$};

  %Emission lines
  \draw [->, to path={-| (\tikztotarget)}] (h1) edge[out=-90,in=90] (e1);
  \draw [->, to path={-| (\tikztotarget)}] (h2) edge[out=-90,in=90] (e2);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=-90,in=90] (etp1);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=-90,in=90] (et);
  \draw [->, to path={-| (\tikztotarget)}] (htf1) edge[out=-90,in=90] (etf1);

  %1. order lines
  \draw [->, to path={-| (\tikztotarget)}] (h1) edge[out=0,in=180] (h2);
  \draw [->, to path={-| (\tikztotarget)}] (h2) edge[out=0,in=180] (dots);
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=0,in=180] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=0,in=180] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=0,in=180] (htf1);
\end{tikzpicture}
\caption[Absolute and relative time trellis diagram for a HMM]{An absolute and relative time trellis diagram for a 1st order HMM}\label{fig:4thMarkovModel}
\end{figure}

Now that we have a complete model of the problem domain, we can move on to the two tasks, this project will use this model for; learning and prediction.

\subsection{Learning}\label{sec:learner}
Learning is the task of determining the parameters of a HMM. Remember that we cannot reliably learn the structure (the states) and probabilities of the HMM. Therefore we, in the previous section, discussed how to determinate the states.

For learning, this project uses the Baum-Welch algorithm\cite{hmmIntroduction}. This section will explain how the Baum-Welch algorithm is computed.

Given the model $\theta = \{A, B, \pi\}$ and a sequence of observations $\mathcal{O}$, the algorithm tries to find the parameters $A$, $B$ and $\pi$ that maximises the probability of the observations given as input, that is $P(Y | \theta)$. It is important to note that the algorithm is only guaranteed to find a local maxima.

The algorithm can be summarised in the following points\cite{hmmIntroduction}.

\begin{enumerate}
\item Initialise $A$, $B$ and $\pi$
\item Compute $\alpha_t(i)$, $\beta_t(i)$, $\gamma_t(i,t)$ and $\gamma_t(i)$
\item Re-estimate the parameters $A$, $B$ and $\pi$
\item If $P(\mathcal{O} | \theta)$ increases, or the number of iterations has not exceeded a maximum number of iteration, repeat from step 2
\end{enumerate}

The idea of the algorithm is to start with some arbitrary parameters for the model, $A$, $B$ and $\pi$. Based on observations, the probabilities for each state in the model for every time $t$ can be calculated. The equations below describe how this is calculated. Based on this estimation of probabilities, a better estimate on the parameters for the model can be calculated. This is also described below. This cycle is repeated until the parameters converge to some values.

The new parameters can be calculated in the following way\cite{hmmIntroduction}.

\begin{equation*}
\pi_i = \sum\limits_{j=1}^{N} \gamma_0(j)
\end{equation*}

\begin{equation*}
a_{ij} = \frac{\sum\limits_{t=1}^{T-1} \gamma_t(i, j)}{ \sum\limits_{t=1}^{T-2} \gamma_t(i) }
\end{equation*}

\begin{equation*}
b_j(k) = \frac{ \sum\limits_{t \in \{1,\ldots, T\}, \mathcal{O}_t = k} \gamma_t(j) } { \sum\limits_{t=1}^{T} \gamma_t(j) }
\end{equation*}

In the above formula, $\gamma_t(i)$ and $\gamma_t(i, j)$ can be calculated in the following way.

$\gamma_t(i, j)$ is the probability of being in state $x_i$ at time $t$ and transitioning to state $x_j$ at time $t+1$.

\begin{equation*}
\gamma_t(i,j) = \frac { \alpha_t(i) a_{ij} b_j(\mathcal{O}_{t+1}) \beta_{t+1}(j) } { \sum\limits_{k=1}^{T} \alpha_{T}(k) }
\end{equation*}

$\gamma_t(i)$ is the probability of being in state $x_i$ at time $t$ given the observed sequence $\mathcal{O}$ and the parameters $\theta$.

\begin{equation*}
\gamma_t(i) = \sum\limits_{j=1}^{N} \gamma_t(i,j)
\end{equation*}

$\alpha_t(i)$ is the probability of the partial observation sequence up to time $t$, where the hidden state is in $x_i$ at time $t$. $\alpha_t(i)$ can be computed using the following recursive formulae.

\begin{align*}
\alpha_0(i) &= \pi_i b_i(\mathcal{O}_1) \\
\alpha_t(i) &= \Bigg( \sum\limits_{j=1}^{N-1} \alpha_{t-1}(j) a_{ji} \Bigg) b_i(\mathcal{O}_t)
\end{align*}

$\beta_t(i)$ is the probability of the ending observation sequence $o_t,\ldots,o_{T}$ given starting state $i$ at time $t$.
\begin{align*}
\beta_{T}(i) &= 1 \\
\beta_t(i) &= \sum\limits_{j=1}^{N} a_{ij} b_j(\mathcal{O}_{t+1}) \beta_{t+1}(j)
\end{align*}

\subsection{Time as a sensor}
Another possible property to include in each snapshot is time, but just using linear time\footnote{e.g. time from to Unix Epoch\cite{unix_epoch}} as a feature of the state will not benefit the model, since two states will never equivalent by linear time. But if we instead use divisions of this time, we can consider two instances of time as the same, e.g. the hour of the day, the day of the week. Now the learner can learn on these features if relevant or not for the users pattern. E.g an user always does the laundry at around 12:00 o'clock on Monday, for this pattern, the month and year divisions will be consider as irrelevant for the behavioural pattern. These time formats should be found prior to implementation of the system and be based on studies on human perceptions of time. Figuring if a division of time is relevant on a pattern and the system only has two weeks of samples, then nothing can be said about the day of the week yet, since there are only instances of repeating days.

An implementation of these time divisions would act as any other sensor in the system. It was decided that the variance of the sensors is already sufficient for illustrating the concept behind the project. Since these time divisions take considerably longer to find, and are more error prone compared to other sensors, it was chosen to delimit the project from this consideration.

\subsection{Prediction}\label{sub:Prediction}
The task of predicting is done after the model has been learned. This is the task of predicting, given a list of observations $\{o_1,\dots,o_t\}$ and a model, what the most likely observation to be emitted in the next time step $x_{t+1}$ is. This can be done, via the Viterbi algorithm\cite{russell2010artificial}. From a sequence of observations, the Viterbi algorithm is used to find the most probable sequence of hidden states, that caused this list of observations to occur, i.e. what sequence of hidden states $\{x_1,\dots,x_t\}$ is most probable to have emitted the observation sequence.

From this we can then predict what is most likely going to be the next hidden state, and then from this, what is most likely going to be emitted. The whole point of doing this is that we can now look at the difference in what is currently true in the problem domain, and what is most likely going to happen the next future. From this we can find out if we can emulate any of this, and if the probability of what is going to happen next is high enough, i.e. our confidence of what is going to happen next, we can emulate it. This means that the system does the action, which the user would do next, before the user has to.
