\section{Hidden Markov Model}
The only information available for the machine intelligence model of the project are the sensors of the problem domain and time. For this we will use the term -\emph{snapshot}. A snapshot is here meant as the values of all sensors in the home at the same time, snapshot will from here on adhere to this definition. 

To model the machine intelligence the Markov model was chosen. This model en-copes discrete time changes in the states of a world. This is an nearly exact match to this project domain. Further more the model assumes a Newtonian world view. This means what is going to happen in the future is only dependent on what is currently true en the present, not what has happened in the past. We will now introduce the diagrams mainly used to illustrate how the models changes over time graphically. These diagrams are called Trellis diagrams. The simplest form of a Markov model can be seen in \cref{fig:1stMarkovModel}. Here we can see the property that at any state in time $s_t$ the next discrete time future $s_{t+1}$ is only dependent on the current state $s_t$.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  %World States Nodes
  \node          (dots) [draw=none,  minimum width=1.25cm, minimum height=.75cm] {\LARGE \dots};
  \node[ellipse] (htp2) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of dots] {$s_{t-2}$};
  \node[ellipse] (htp1) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of htp2] {$s_{t-1}$};
  \node[ellipse] (ht)   [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of htp1] {$s_t$};
  \node[ellipse] (htf1) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of ht] {$s_{t+1}$};
  
  %1. order transition lines
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=0,in=180] (htp2);
  \draw [->, to path={-| (\tikztotarget)}] (htp2) edge[out=0,in=180] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=0,in=180] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=0,in=180] (htf1);
\end{tikzpicture}
\caption[Trellis diagram for a simple Markov model]{The trellis diagram for a hidden Markov model with descrete time world state changes}\label{fig:1stMarkovModel}
\end{figure}
Even though these diagrams are good at illustrating the models and how they change over time it is important to remember that these are not illustrating the exact models themselves, only how they change over time for a specific sequence of observations. The actual model could be illustrated via causal/Bayesian networks, which also will be used later.

Even though the Markov model is this is a good fit, problems arises when using it. These problem arises since the model has a Newtonian world view. One could argue that this world view is also applicable for our problem domain; what the user is going to do is only dependent on the current physical state of the world and what the user is current thinking and remembering. The problem is that we can not sensor everything about the current complete physical state, or at least not everything affecting his/her decision, and differently not what the user is thinking and remembering.

We start by trying to encompass the current complete physical state that effect the users routines in their homes. The problem here is that this is individual for different users and to observe this, one would need enough sensors to encompass all individual peoples needs. This is very hard to find, even if everyone could be asked or represented, and everyone would need to since the project do not want to focus on a specific group of people, people do not necessarily know what factors affect their choises. Furthermore new and prior unknown use patterns could emerge after the implementation either because of the system or something externally or future users could simply not yet borne. Some sensors needed could simply not exist or be too imprecise or impractical. In short, finding all sensors needed to being able to observe all data needed for everyoneâ€™s use pattern is impossible in practice. To encompass this the we now introduce the hidden Markov model (HMM). The hidden states of the HMM are introduced to represent the exact reality, that cannot be observed. A hidden state can then emit an observable state. In this project this means that the hidden states are the true state of the world, all information needed for the user to take a choice on what to do next. This unobservable world then emits values to the sensors that we can observe, creating the observable states. Now the problem is however that these hidden states are all possible complete worlds and would in practice be infinitely many. The solution for this we find in the second problem. To illustrate the HMM we look to \cref{fig:2ndMarkovModel}. Here we can see that any given hidden state emits an observable state at any given time. 


\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  %Hidden Nodes
  \node          (dots) [draw=none,  minimum width=1.25cm, minimum height=.75cm] {\LARGE \dots};
  \node[ellipse] (htp2) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of dots] {$x_{t-2}$};
  \node[ellipse] (htp1) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of htp2] {$x_{t-1}$};
  \node[ellipse] (ht)   [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of htp1] {$x_t$};
  \node[ellipse] (htf1) [draw=black, minimum width=1.25cm, minimum height=.75cm, right=of ht] {$x_{t+1}$};

  %Emission states
  \node[ellipse] (etp1) [draw=black, minimum width=1.25cm, minimum height=.75cm, below=of htp1] {$e_{t-1}$};
  \node[ellipse] (etp2) [draw=black, minimum width=1.25cm, minimum height=.75cm, below=of htp2] {$e_{t-1}$};
  \node[ellipse] (et)   [draw=black, minimum width=1.25cm, minimum height=.75cm, below=of ht] {$e_t$};
  \node[ellipse] (etf1) [draw=black, minimum width=1.25cm, minimum height=.75cm, below=of htf1] {$e_{t+1}$};

  %Separator
  \coordinate (MW) at (0,-0.8);
  \node (HW) [above left=0ex of MW] {Hidden world $\uparrow$};
  \node (VW) [below left=0ex of MW] {Oservable world $\downarrow$};
  \draw[dashed, draw=black] (-1,-0.8) -- (10.5,-0.8);
  %\draw[dashed] ($(htp2)!0.5!(etp2)$) -- ($(htf1)!0.5!(etf1)$);
  
  %Emission lines
  \draw[->, to path={-| (\tikztotarget)}] (htp1) edge[out=-90,in=90] (etp1);
  \draw[->, to path={-| (\tikztotarget)}] (htp2) edge[out=-90,in=90] (etp2);
  \draw[->, to path={-| (\tikztotarget)}] (ht) edge[out=-90,in=90] (et);
  \draw[->, to path={-| (\tikztotarget)}] (htf1) edge[out=-90,in=90] (etf1);
  
  %1. order lines
  \draw[->, to path={-| (\tikztotarget)}] (dots) edge[out=0,in=180] (htp2);
  \draw[->, to path={-| (\tikztotarget)}] (htp2) edge[out=0,in=180] (htp1);
  \draw[->, to path={-| (\tikztotarget)}] (htp1) edge[out=0,in=180] (ht);
  \draw[->, to path={-| (\tikztotarget)}] (ht) edge[out=0,in=180] (htf1);
\end{tikzpicture}
\caption[Trellis diagram for a simple Hidden Markov Model]{The trellis diagram for a simple hidden Markov model that implements a Newtonian worldveiw}\label{fig:2ndMarkovModel}
\end{figure}

To encompass what the user is currently thinking and remembering, essentially depend on what happened in the past and what is currently true. We therefore need a way to en-cope the past in the current state. This is called the order of HMM. The order of a HMM directly corresponds to how much any state $s_t$ is dependent on. This is better illustrated, see \cref{fig:3rdMarkovModel}.


\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  %Hidden Nodes
  \node          (dots) [draw=none,  minimum width=1cm, minimum height=.75cm] {\LARGE \dots};
  \node[ellipse] (htp2) [draw=black, minimum width=1cm, minimum height=.75cm, right=of dots] {$x_{t-2}$};
  \node[ellipse] (htp1) [draw=black, minimum width=1cm, minimum height=.75cm, right=of htp2] {$x_{t-1}$};
  \node[ellipse] (ht)   [draw=black, minimum width=1cm, minimum height=.75cm, right=of htp1] {$x_t$};
  \node[ellipse] (htf1) [draw=black, minimum width=1cm, minimum height=.75cm, right=of ht] {$x_{t+1}$};

  %Emission states  
  \node[ellipse] (etp2) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htp2] {$e_{t-2}$};
  \node[ellipse] (etp1) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htp1] {$e_{t-1}$};
  \node[ellipse] (et)   [draw=black, minimum width=1cm, minimum height=.75cm, below=of ht] {$e_t$};
  \node[ellipse] (etf1) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htf1] {$e_{t+1}$};
  
  %Emission lines
  \draw [->, to path={-| (\tikztotarget)}] (htp2) edge[out=-90,in=90] (etp2);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=-90,in=90] (etp1);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=-90,in=90] (et);
  \draw [->, to path={-| (\tikztotarget)}] (htf1) edge[out=-90,in=90] (etf1);
  
  %1. order lines
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=0,in=180] (htp2);
  \draw [->, to path={-| (\tikztotarget)}] (htp2) edge[out=0,in=180] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=0,in=180] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=0,in=180] (htf1);
  
  %2. order lines
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=45,in=135] (htp2);
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=45,in=135] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (htp2) edge[out=45,in=135] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=45,in=135] (htf1);
\end{tikzpicture}
\caption[Trellis diagram for a 2nd order Hidden Markov Model]{A trellis diagram for a 2nd order hidden Markov model.}\label{fig:3rdMarkovModel}
\end{figure}

\Cref{fig:3rdMarkovModel} shows the trellis diagram for a 2nd order hidden Markov model. Here we can see that every state $s_t$ is dependent on two states in the past $s_{t-1}$ and $s_{t-2}$ meaning that every states is not only dependent on what happened the time step before. This can trivially be expanded to any nth order. One would probably notice now that every model we have look at so far has been a 1st order Markov model. Any nth order Markov model where $n>1$ violates the Newtonian world-view, also known as the Markov property. Many algorithms within this field assumes this Markov property, including the ones later used in this project. Any nth order Markov model can be converted to a 1st order Markov model, thereby keeping the Markov property. This is in this project done by encoding the needed past in the hidden state together with the current state.

On the basis of this we can now find the specific hidden states. We introduce the term sample that covers the current obervable state with the history of snapshots n steps back. The size of this n will be discussed in \cref{}\jenote{need reference to section about the sampler}. Since the history is en-coded in the states we can only differentiate between a history of snapshots. A hidden state will therefore be created for every sample in the HMM.

The formal definition of a HMM is $\theta=\{A,B,\pi\}|A = \text{transition matrix}, B = \text{emission matrix}, \pi = \text{initial distribition}$. The transition matirix is the matrix that.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[->]
  %Hidden Nodes
  \node[circle, draw=black, minimum size=1cm] (x1) {$x_1$};
  \node[circle, draw=black, minimum size=1cm, node distance=3cm, right=of x1] (x2) {$x_2$};
  \node[circle, draw=black, minimum size=1cm, node distance=3cm, below=of x1] (x3) {$x_3$};
  \node[circle, draw=black, minimum size=1cm, node distance=3cm, below=of x2] (x4) {$x_4$};

  %Emission states
  \node[circle] (e11) [draw=black, minimum size=1cm, node distance=2cm, above=of x1] {$e_1$};
  \node[circle] (e12) [draw=black, minimum size=1cm, node distance=2cm, left=of x1] {$e_2$};
  \node[circle] (e21) [draw=black, minimum size=1cm, node distance=2cm, above=of x2] {$e_1$};
  \node[circle] (e22) [draw=black, minimum size=1cm, node distance=2cm, right=of x2] {$e_2$};
  \node[circle] (e31) [draw=black, minimum size=1cm, node distance=2cm, left=of x3] {$e_1$};
  \node[circle] (e32) [draw=black, minimum size=1cm, node distance=2cm, below=of x3] {$e_2$};
  \node[circle] (e41) [draw=black, minimum size=1cm, node distance=2cm, below=of x4] {$e_1$};
  \node[circle] (e42) [draw=black, minimum size=1cm, node distance=2cm, right=of x4] {$e_2$};
  
  \path
  %Emission lines
    (x1) edge[out=90,in=-90] node[rectangle,fill=white] {$b_{11}$} (e11)
    (x1) edge[out=180,in=0] node[rectangle,fill=white] {$b_{12}$} (e12)
    (x2) edge[out=90,in=-90] node[rectangle,fill=white] {$b_{21}$} (e21)
    (x2) edge[out=0,in=180] node[rectangle,fill=white] {$b_{22}$} (e22)
    (x3) edge[out=180,in=0] node[rectangle,fill=white] {$b_{31}$} (e31)
    (x3) edge[out=-90,in=90] node[rectangle,fill=white] {$b_{32}$} (e32)
    (x4) edge[out=-90,in=90] node[rectangle,fill=white] {$b_{41}$} (e41)
    (x4) edge[out=0,in=180] node[rectangle,fill=white] {$b_{42}$} (e42)

  %Transition lines
    (x1) edge[out=15,in=165] node[rectangle,fill=white] {$a_{12}$} (x2)
    (x2) edge[out=-165,in=-15] node[rectangle,fill=white] {$a_{21}$} (x1)
    (x2) edge[out=-75,in=75] node[rectangle,fill=white] {$a_{24}$} (x4)
    (x4) edge[out=105,in=-105] node[rectangle,fill=white] {$a_{42}$} (x2)
    (x4) edge[out=-165,in=-15] node[rectangle,fill=white] {$a_{43}$} (x3)
    (x3) edge[out=15,in=165] node[rectangle,fill=white] {$a_{34}$} (x4)
    (x3) edge[out=105,in=-105] node[rectangle,fill=white] {$a_{31}$} (x1)
    (x1) edge[out=-75,in=75] node[rectangle,fill=white] {$a_{13}$} (x3)
  
    (x1) edge[out=-30,in=120] node[rectangle,fill=white] {$a_{14}$} (x4)
    (x4) edge[out=150,in=-60] node[rectangle,fill=white] {$a_{41}$} (x1)
    (x3) edge[out=60,in=-150] node[rectangle,fill=white] {$a_{32}$} (x2)
    (x2) edge[out=-120,in=30] node[rectangle,fill=white] {$a_{23}$} (x3)

    (x1) edge [out=150,in=120,loop,looseness=10] node[rectangle,fill=white] {$a_{11}$} (x1)
    (x2) edge [out=60,in=30,loop,looseness=10] node[rectangle,fill=white] {$a_{22}$} (x2)
    (x3) edge [out=-120,in=-150,loop,looseness=10] node[rectangle,fill=white] {$a_{33}$} (x3)
    (x4) edge [out=-30,in=-60,loop,looseness=10] node[rectangle,fill=white] {$a_{44}$} (x4);
\end{tikzpicture}
\caption[Caption]{Bla bla}\label{fig:monolithic_system}
\end{figure}

We therefore need to en-cope the history in the hidden states of the model. 

the possibly to en-cope unobserved variables in the states via hidden states (explained later in this section)

An initial implementation of a Markov chain in our problem domain could make the states in the model snapshots of the sensors in the home. Here two problems arises. First, what a person decides to do next is not only dependent on what he or she is currently doing, also what has happen in the past.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  %Hidden Nodes
  \node[ellipse] (h1)   [draw=black, minimum width=1cm, minimum height=.75cm] {$x_1$};
  \node[ellipse] (h2)   [draw=black, minimum width=1cm, minimum height=.75cm, right=of h1] {$x_2$};
  \node          (dots) [draw=none,  minimum width=1cm, minimum height=.75cm, right=of h2] {\LARGE \dots};
  \node[ellipse] (htp1) [draw=black, minimum width=1cm, minimum height=.75cm, right=of dots] {$x_{t-1}$};
  \node[ellipse] (ht)   [draw=black, minimum width=1cm, minimum height=.75cm, right=of htp1] {$x_t$};
  \node[ellipse] (htf1) [draw=black, minimum width=1cm, minimum height=.75cm, right=of ht] {$x_{t+1}$};

  %Emission states
  \node[ellipse] (e1)   [draw=black, minimum width=1cm, minimum height=.75cm, below=of h1] {$e_1$};
  \node[ellipse] (e2)   [draw=black, minimum width=1cm, minimum height=.75cm, below=of h2] {$e_2$};
  \node[ellipse] (etp1) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htp1] {$e_{t-1}$};
  \node[ellipse] (et)   [draw=black, minimum width=1cm, minimum height=.75cm, below=of ht] {$e_t$};
  \node[ellipse] (etf1) [draw=black, minimum width=1cm, minimum height=.75cm, below=of htf1] {$e_{t+1}$};
  
  %Emission lines
  \draw [->, to path={-| (\tikztotarget)}] (h1) edge[out=-90,in=90] (e1);
  \draw [->, to path={-| (\tikztotarget)}] (h2) edge[out=-90,in=90] (e2);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=-90,in=90] (etp1);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=-90,in=90] (et);
  \draw [->, to path={-| (\tikztotarget)}] (htf1) edge[out=-90,in=90] (etf1);
  
  %1. order lines
  \draw [->, to path={-| (\tikztotarget)}] (h1) edge[out=0,in=180] (h2);
  \draw [->, to path={-| (\tikztotarget)}] (h2) edge[out=0,in=180] (dots);
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=0,in=180] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=0,in=180] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (ht) edge[out=0,in=180] (htf1);
  
  %2. order lines
  \draw [->, to path={-| (\tikztotarget)}] (h1) edge[out=45,in=135] (dots);
  \draw [->, to path={-| (\tikztotarget)}] (h2) edge[out=45,in=135] (dots);
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=45,in=135] (htp1);
  \draw [->, to path={-| (\tikztotarget)}] (dots) edge[out=45,in=135] (ht);
  \draw [->, to path={-| (\tikztotarget)}] (htp1) edge[out=45,in=135] (htf1);
\end{tikzpicture}
\caption[Caption]{Bla bla}\label{fig:monolithic_system}
\end{figure}

A markov chain can have multiple orders. This is how many . Another problem also arises when using markov models. Of how high order should the model be? A first order 

 In a markov chain.
Hidden states: The real pattern the user is currently doing.
Evidence states: Snapshots of the problem domain.

Hidden Markov chains are the simplest form of

\subsection{Learning}
For learning the Baum-Welch algorithm, a specific implementatian of the forwards-backwards algorithm, was choosen.

\subsection{Prediction}
