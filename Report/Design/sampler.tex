%mainfile: ../master.tex
\section{Sampler}\label{sec:sampler}
The job of the sampler is to map the current sensor values, with a given history of snapshots, into some input features for the learner.
The idea is to find correlations between sensor values at a given time $t$, rather than finding correlations between snapshots from time $t_{n-k}$ to $t_n$. This is also the reason for making a new state scope\kanote{new term} everytime a new normalised snapshot is received, as first order Markov models inherently have the Markov property\footnote{The probability distribution of the next state depends only on the current state and not on the sequence of events that preceded it.\cite{wiki_markov_chain}}. To overcome this problem, the history of how the system arrived at this state is encoded in the input feature for the learner. Why the Markov assumption\kanote{Markov property? If so, why have we argued that we modify our model in order to have the property?} is not preferable in the context of this system is discussed in this paper\cite{Allahviranloo201316} by Mahdieh Allahviranloo and Will Recker, it mentions that state $t+1$ is potentialy dependant on the whole history of states.

Some research on how many states a usage pattern streches over, should be investigated, in order to find how many states, should be included in the scopes.

\subsection{Emulatable Actions}
One can infer that an action has happened in the problem domain, when an observed sensor value, which is also emulatable, changes from the state, given at time $t-1$ to the state given at time $t$. The sampler can then, along with the history of the states, that lead to this infered action, pass the action itself, to the learner. The format of an action is a 3-tuple, $(S,V_1,V_2)$ describing sensor $S$ affected by values $V_1, V_2$(the sensors values at time $t-1$ and $t$).

\subsection{Unique Identifier for Snapshot}

In order to differentiate one snapshot from another, without having to observe each individual sensor value in the snapshots, a unique identifier for the snapshot is required.
Thi approach has some benefits, but also some limitations. The learning algorithm has less of an opportunity to overfit on a certain noisy sensor value in the problem domain, in that it can not directly observe the individual sensor values from each other.
Another benefit is that by concatenating all the sensor values into one feature, we reduce the number of features the learner has as input for the problem, on the other hand we increase the value domain of each of the features, to differentiate snapshot from each other.

\subsection{Loss of Resolution}
The major disadvantage is that states that are closely similar in terms of sensor value, might not be close by the unique identifier, which means that clustering sensor states by the numeric value of their identifiers, might not make sense, due to not having an ordering or localisation between the assigned unique values.
The aforementioned note on overfitting, might also reveal itself as not being a problem at all, as were the individual sensor values as features for the learner might be beneficial to reasoning properly in the problem domain\kanote{I don't get this point}.
