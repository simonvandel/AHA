\section{User Interfaces}
There are two kinds of user interaction in the system: passive and active interaction.

\subsection{Passive User Interaction}
Passive user interactions are those interactions that the users make with the environment that are silently monitored by the system to infer some emerging behaviour of the user. Examples of passive interactions are: \enquote{the user turns on the spotlight right after turning on the TV}, \enquote{the user turns on the coffee machine at 6:30}.

\subsection{Active User Interaction}
Active interactions are all those interactions that the user consciously has with the system. Examples of such interactions are: \enquote{the user tells the system that the coffee machine should not be turned on at 6:30 in the weekends}, \enquote{the user informs the system that the home will be vacant the next week}.

\subsection{Design of User Interface}
As stated in \cref{sec:requirements}, the system should be as invisible to the user as possible. This means that passive user interaction is to be preferred whenever possible. 

\subsubsection{Informing the system of wrong actions}

The system has to gradually learn which actions suits the user the best. To learn, the system has to have a fitness function to evaluate how well its performed actions were. The best input for such a fitness function is the user's opinion. However, this function can not extracted the input it needs from passive user interaction. The user has to actively interact with the system, telling the system how appropriate its actions were. Therefore, a mix of active and passive interaction is needed between the user and the system.

To reduce active user interaction, the user should only inform the system when it has made a mistake. The interface could also be able to inform the system when it has made a correct action, but requiring the user to inform the system too often, is distracting for the user. Therefore, if an action is not marked as wrong, it is assumed to be correct. Furthermore, the system should aim at avoiding wrong actions as much as possible. Meaning that the system should be conservative in its actions, so that before the system gets confident to start performing an action it should have acquired a number of observations of that action.

One way the user could inform the system of a mistake, is by reversing the action the system performed badly. For example, if the system wrongly turned off the light in the bathroom, the user would naturally turn on the light again. This would give the system an indication that its action was wrong. This is a good way for the user to inform the system, as it feels natural for the user. However, there are problems with this approach. For example, if the system wrongly turns on the light in the bedroom, but the user first notices this 30 minutes later, and then turns off the light. Because the user does not immediately notice the wrong action, it is ambiguous for the system whether the whole action of turning the light on was wrong, or that it was correct to turn the light on, but to only leave it on for 30 minutes. In addition how long does the system need to incorporate the knowledge that the action were wrong. The exact amount of time to wait for an reversal of an action is difficult to determine it would require experimentation. 

The best user experience is by having the user reverse the wrong action of the user, but in cases where this is not possible, an alternative interface is needed. One possible solution is a graphical user interface that displays all actions the system has made. The user can then choose actions and mark them as wrong. In contrast to the system inferring that an action were wrong because the user reverted it, as described above, this way of informing the system is unambiguous. Unambiguous because even if it took the user 30 minutes to mark the action as wrong the system can not interpret it as a rule instead. However this method of user input is also less convenient for the user.

\subsubsection{Visualising system knowledge}

As discussed in \cref{sec:interviewReports}, one of the problems of the Nest system was that the user could not reason about the actions of the system. That is, they would not know why a given action was made. To alleviate this problem, the knowledge of the system should be transparent. A solution would be to visually represent the knowledge of the system. The earlier paragraph mentioned a graphical user interface for displaying all actions the system has made. This can be extended to also include why a given action was made. For example, the interface could display the action \enquote{light turned on in living room at 21:30}. If the user clicked on this action, the system would display the reasoning behind this action. In this case, it could be: \enquote{The light intensity in the room was below 10 lux}.