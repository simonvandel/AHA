\section{Server implementation - Learner}
\subsection{Baum Welch}
\begin{frame}
	\frametitle{Baum Welch - Principle}
	\begin{itemize}
		\item N = number of hidden states in the model
		\item M = number of emission states in the model
		\item Problem: Given a sequence of observations $\mathcal{O}$ and the dimensions M and N, find the model $\theta = \{A, B, \pi\}$ that maximises the probability for $\mathcal{O}$. That is, the model that is most likely to have emitted the observations
		\item This can be solved via the Baum-Welch algorithm
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Baum Welch - Principle}
	\begin{itemize}
		\item Calculate $\alpha$/forwards and $\beta$/backwards
		\begin{itemize}
			\item $\alpha$ is the probability that the hidden state is q at time t given a partial observation sequence
			\item $\beta$ is the same as $\alpha$ but it starts at the end and works itself backwards
		\end{itemize}
		\item Calculate di-gammas and gamma
		\begin{itemize}
			\item $\gamma(i,j)$ is the probability of being 
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Baum Welch - }
	\begin{itemize}
		\item Calculate di-gammas and gamma
		\begin{itemize}
			\item $y_t(i,j) = P(x_t = q_i, x_{t+1} = q_j | \mathcal{O},\theta)$
			\item $y_t(i,j) = \frac{\alpha_t(i) a_{ij} b_j (\mathcal{O}_{t+1})\beta_{t+1}(j)}{P(\mathcal{O} | \theta)}$
		\end{itemize}
		\item Calculate $\alpha$ and $\beta$
		\begin{itemize}
			\item $\alpha_t(i) = P(\mathcal{O}_0, \mathcal{O}_1, \dots ,\mathcal{O}_t , x_t = q_i | \theta)$
			\item $\alpha_0(i) = \pi_i b_i(\mathcal{O}_0)$ for $i = 0, 1, \dots, N-1$
			\item $\alpha_t(i) = \left(\sum\limits_{j=0}^{N-1} \alpha_{t-1}(j)a_{ji}\right)b_i(\mathcal{O}_t)$
			\item $\beta_t(i) = P(\mathcal{O}_{t+1}, \mathcal{O}_{t+2}, \dots, \mathcal{O}_{T-1} | x_t = q_i, \theta)$
			\item $\beta_{T-1}(i) = 1$
			\item $\beta_t(i) = $
		\end{itemize}
	\end{itemize}
\end{frame}